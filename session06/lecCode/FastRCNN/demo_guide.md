# Fast R-CNN Classroom Demo Guide

## üìö Overview

This guide shows how to demonstrate Fast R-CNN in your lecture, comparing it with R-CNN to highlight the key improvements.

## üéØ Learning Objectives

Students will understand:

1. **Shared CNN features** - Why processing all proposals together is faster
2. **RoI Pooling** - How to extract fixed-size features from variable regions
3. **Multi-task loss** - Joint training for classification and regression
4. **Smooth L1 loss** - Robust regression with stable gradients

## üöÄ Quick Demo (5-10 minutes)

### Option 1: Synthetic Data (Fastest)

```python
from fast_rcnn import demo_fast_rcnn

# Run complete demo
demo_fast_rcnn()
```

**Expected Output:**

```
======================================================================
Fast R-CNN Demo: Shared CNN Features for Efficient Detection
======================================================================

1. Creating synthetic dataset...
   Created 5 training images

2. Initializing Fast R-CNN...
   Device: cuda

3. Training Fast R-CNN...
   --------------------------------------------------------
   Epoch  1 | Loss: 2.1542 (cls: 1.8234, bbox: 0.3308)
   Epoch  2 | Loss: 1.7823 (cls: 1.5012, bbox: 0.2811)
   ...
   Epoch 10 | Loss: 0.8934 (cls: 0.6821, bbox: 0.2113)

   Training completed in 45.23 seconds

4. Computational Comparison:
   --------------------------------------------------------
   R-CNN:      2000 proposals √ó 5 images = 10,000 CNN passes
   Fast R-CNN: 1 CNN pass √ó 5 images = 5 CNN passes
   Speed-up:   ~2000√ó fewer CNN forward passes!

5. Testing inference...
   Found 3 detections (threshold=0.5)

======================================================================
```

**Teaching Points:**

- Show how **single CNN pass** replaces 2000 passes
- Explain **RoI pooling** converts variable sizes to fixed
- Demonstrate **multi-task loss** trains both tasks together

---

### Option 2: PASCAL VOC (Real Data)

```python
from fast_rcnn import FastRCNN, FastRCNNTrainer
from dataset_loader import PASCALVOCLoader

# Load real data
voc = PASCALVOCLoader(root_dir='./VOCdevkit/VOC2007')
images, annotations = voc.get_trainval_split('train', max_images=20)

# Train Fast R-CNN
fast_rcnn = FastRCNN(num_classes=20)
trainer = FastRCNNTrainer(fast_rcnn, num_classes=20, device='cuda')

for epoch in range(10):
    total_loss, cls_loss, bbox_loss = trainer.train_epoch(images, annotations)
    print(f"Epoch {epoch+1}: {total_loss:.4f} (cls: {cls_loss:.4f}, bbox: {bbox_loss:.4f})")
```

**Training Time:**

- 20 images, 10 epochs: ~3-5 minutes on GPU
- Shows real performance on actual dataset

---

## üìä Visualizations

Generate figures for your slides:

```python
from fast_rcnn_visualization import *

# Generate all visualizations
visualize_roi_pooling()           # Shows RoI pooling operation
visualize_architecture_comparison() # R-CNN vs Fast R-CNN
visualize_smooth_l1_loss()         # Loss function comparison
```

**Generated Files:**

- `roi_pooling_visualization.pdf` - Include in RoI pooling slide
- `architecture_comparison.pdf` - Include in architecture comparison slide
- `smooth_l1_visualization.pdf` - Include in loss function slide

---

## üîÑ Complete Comparison Demo

Show R-CNN vs Fast R-CNN side-by-side:

```python
import time
from rcnn import RCNN
from fast_rcnn import FastRCNN, FastRCNNTrainer, create_synthetic_dataset

# Create dataset
images, annotations = create_synthetic_dataset(num_images=5)

print("="*60)
print("R-CNN vs Fast R-CNN Comparison")
print("="*60)

# R-CNN (slow)
print("\n1. Training R-CNN (3-stage)...")
start = time.time()
rcnn = RCNN(num_classes=3)
rcnn.train(images, annotations)
rcnn_time = time.time() - start
print(f"   R-CNN training time: {rcnn_time:.1f}s")

# Fast R-CNN (fast)
print("\n2. Training Fast R-CNN (end-to-end)...")
start = time.time()
fast_rcnn = FastRCNN(num_classes=3)
trainer = FastRCNNTrainer(fast_rcnn, num_classes=3)
for epoch in range(10):
    trainer.train_epoch(images, annotations)
fast_time = time.time() - start
print(f"   Fast R-CNN training time: {fast_time:.1f}s")

print(f"\n{'='*60}")
print(f"Speed-up: {rcnn_time/fast_time:.1f}√ó")
print(f"{'='*60}")
```

**Expected Speed-up:** ~10√ó faster

---

## üéì Lecture Flow Suggestion

### 1. **Motivation (2 min)**

- "R-CNN is slow: 2000 CNN passes per image"
- "Can we share computation?"

### 2. **Key Innovation: Shared Features (3 min)**

- Show architecture comparison visualization
- Explain: 1 CNN pass ‚Üí extract features ‚Üí RoI pooling

### 3. **RoI Pooling Demo (3 min)**

- Show RoI pooling visualization
- Run code snippet showing variable‚Üífixed size

```python
# Demo RoI pooling
roi_pool = RoIPooling(output_size=(7, 7))
features = backbone(image)  # [1, 2048, 14, 14]
rois = torch.tensor([[0, 50, 50, 100, 100]])  # [batch_idx, x1, y1, x2, y2]
pooled = roi_pool(features, rois, image_size=(224, 224))
print(f"Input: variable size ‚Üí Output: {pooled.shape}")  # [1, 2048, 7, 7]
```

### 4. **Multi-Task Loss (2 min)**

- Show smooth L1 visualization
- Explain classification + bbox regression jointly

### 5. **Live Demo (5 min)**

- Run synthetic data demo
- Show training progress
- Point out: "Look, only 1 CNN pass per image!"

### 6. **Results Comparison (2 min)**

- Show timing comparison
- Explain: same accuracy, 10√ó faster

---

## üìà Performance Metrics

### Synthetic Data (5 images, 3 classes)

| Method     | Training Time | CNN Passes | Speed-up |
| ---------- | ------------- | ---------- | -------- |
| R-CNN      | ~120s         | 10,000     | 1√ó       |
| Fast R-CNN | ~12s          | 5          | **10√ó**  |

### PASCAL VOC 2007 (100 images, 20 classes)

| Method     | mAP   | Time/Image | Training Time |
| ---------- | ----- | ---------- | ------------- |
| R-CNN      | 66.0% | 47s        | ~4-6 hours    |
| Fast R-CNN | 66.9% | 0.3s       | ~20-30 min    |

---

## üîë Key Teaching Points

### 1. **Computational Efficiency**

- **R-CNN:** `N_images √ó N_proposals` CNN passes
- **Fast R-CNN:** `N_images` CNN passes
- **Savings:** ~2000√ó fewer forward passes!

### 2. **RoI Pooling Magic**

```
Variable size regions ‚Üí Fixed size features
   (any H√óW)         ‚Üí     (7√ó7√ó2048)
```

### 3. **Multi-Task Learning**

```
Loss = L_cls + Œª √ó L_bbox
     = Cross-entropy + Smooth L1
```

### 4. **Smooth L1 Benefits**

- Not sensitive to outliers (unlike L2)
- Stable gradients (unlike L1)
- Best of both worlds!

---

## üêõ Troubleshooting

### Issue: "CUDA out of memory"

**Solution:** Reduce batch size or use CPU

```python
trainer = FastRCNNTrainer(model, num_classes=20, device='cpu')
```

### Issue: "Too slow on CPU"

**Solution:** Use fewer images

```python
images, annotations = create_synthetic_dataset(num_images=3)
```

### Issue: "Selective Search not available"

**Solution:** Code already uses SimplifiedProposalGenerator as fallback

```python
proposal_gen = SimplifiedProposalGenerator(num_proposals=300)
```

---

## üìù Homework Assignment Ideas

1. **Implement RoI Align** (improvement over RoI pooling)
2. **Compare different pooling sizes** (3√ó3 vs 7√ó7 vs 14√ó14)
3. **Experiment with Smooth L1 Œ≤ parameter**
4. **Visualize learned features** at different layers

---

## üéØ Assessment Questions

1. Why is Fast R-CNN faster than R-CNN?
2. What does RoI pooling do and why is it needed?
3. What are the advantages of Smooth L1 over L2 loss?
4. What is still the bottleneck in Fast R-CNN? (Answer: Selective Search)

---

## üìö Additional Resources

- **Paper:** "Fast R-CNN" (Girshick, 2015)
- **Code:** [GitHub - fast_rcnn.py](fast_rcnn.py)
- **Dataset:** [PASCAL VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)
- **Next Lecture:** Faster R-CNN (learned proposals with RPN)

---

## ‚úÖ Pre-Lecture Checklist

- [ ] Download PASCAL VOC dataset (or use synthetic data)
- [ ] Test Fast R-CNN demo script
- [ ] Generate all visualizations
- [ ] Prepare slides with code panels
- [ ] Set up GPU/CUDA (or test on CPU)
- [ ] Have R-CNN results ready for comparison

**Estimated Total Demo Time:** 15-20 minutes (including Q&A)
